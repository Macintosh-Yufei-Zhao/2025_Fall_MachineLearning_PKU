{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7b3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11055a2",
   "metadata": {},
   "source": [
    "# Problem 1: Linear Regression.\n",
    "\n",
    "Data: (2,1)-1; (4,2)-3; (5,4)-4.\n",
    "\n",
    "**Question 1**: Find the closed-form solution of the regressoion.\n",
    "\n",
    "**Answer**: \n",
    "Name $$X=\\begin{pmatrix}2&1&1\\\\4&2&1\\\\5&4&1\\end{pmatrix}$$ and $$Y=(1,3,4)^T,W=(w_1,w_2,b)^T$$\n",
    "The solution is $$W=(X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd8d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 0.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[2,1,1],[4,2,1],[5,4,1]])\n",
    "Y=np.array([[1],[3],[4]])\n",
    "W=np.linalg.inv(X.T@X)@X.T@Y\n",
    "print(W)\n",
    "# Linear Regression using Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ae3c7",
   "metadata": {},
   "source": [
    "The answer is $w=(1,0)^T,b=-1$.\n",
    "\n",
    "**Question 2**: Adding a dimension with data 2,4,8 , is there a unique solution?\n",
    "\n",
    "**Answer**: No. The third dimension caused repeated columns (scaled by 2) in matrix X.\n",
    "\n",
    "**Question 3**: Find the Ridge Regression with $\\lambda=1$.\n",
    "\n",
    "**Answer**: Name $$X=\\begin{pmatrix}2&1&2&1\\\\4&2&4&1\\\\5&4&8&1\\end{pmatrix}$$ and $$Y=(1,3,4)^T,W=(w_1,w_2,b)^T$$\n",
    "The solution is $$W=(X^TX+I)^{-1}X^TY$$\n",
    "The answer is $w\\approx(0.419,0.100,0.201)^T,b\\approx0.031$ (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899630bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41929134]\n",
      " [ 0.1003937 ]\n",
      " [ 0.2007874 ]\n",
      " [-0.03149606]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[2,1,2,1],[4,2,4,1],[5,4,8,1]])\n",
    "Y=np.array([[1],[3],[4]])\n",
    "I=np.eye(4)\n",
    "W=np.linalg.inv(X.T@X+I)@X.T@Y\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a36127",
   "metadata": {},
   "source": [
    "# Problem 2: Weighted Linear Regression.\n",
    "\n",
    "**Question 1**: Derive the closed-form solution.\n",
    "\n",
    "**Answer**: Define $R=diag\\{\\sqrt{r_i}\\}$, then the solution can be written by replacing $X$ by $XR$:\n",
    "$$W=(XRR^TX^T+\\lambda I)^{-1}R^TX^TY$$\n",
    "\n",
    "**Question 2**: The meaning of Weight. Is it equivalent to adding some point for n times?\n",
    "\n",
    "**Answer**: Yes, it is equivalent. Both operation add $r_i$ terms to the squared loss term. This weight reduces the size of the matrices and simplifies calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a8705",
   "metadata": {},
   "source": [
    "# Problem 3: Logistic Regression with soft labels.\n",
    "\n",
    "**Question**: Write down the cross-entropy loss. How do you explain it with MLE?\n",
    "\n",
    "**Answer**: Suppose we have a prediction of $(x_i,p(x_i))$. The cross-entropy loss is:\n",
    "$$H(t,p)=-\\sum_i t_i\\log p(x_i)-\\sum_i (1-t_i)\\log (1-p(x_i))$$\n",
    "Explanation is, the probability of correct prediction is \n",
    "$$P=\\prod_i p(x_i)^{t_i} (1-p(x_i))^{1-t_i}$$\n",
    "and to maximize this probability we need to minimize its negative logarithm which is the cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39732da",
   "metadata": {},
   "source": [
    "# Problem 4: Logistic Regression with L2 Regularization\n",
    "\n",
    "**Question 1**: Prove that scaling a separating hyperplane still gives an MLE solution.\n",
    "\n",
    "**Answer**: Scaling the hyperplane does not affect its shape and location in the hyperspace. As long as the hyperplane is a perfect separating hyperplane, the scaled loss is 0 and the likelihood is 1, which is of course an MLE solution.\n",
    "\n",
    "**Question 2**: Why L2 regularization gives a unique solution?\n",
    "\n",
    "**Answer**: The sigmoid loss itself is convex in some directions, and the norm loss is convex in all directions. The sigmoid loss will never balance the norm loss when the norm goes large, so the whole loss function must be convex in all directions, which gives a unique solution. As an infinite solution will give an infinity norm loss, the final solution will never be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b4544",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Python matrix calculation cheetsheetï¼š\n",
    "![Cheetsheet](./Cheetsheet.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
